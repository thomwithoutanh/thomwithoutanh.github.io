<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2019-01-23T11:44:39+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Tom Walker</title><subtitle>Researcher and writer
</subtitle><author><name>Tom Walker</name><email>&lt;hello@tomwalker.fyi&gt;</email></author><entry><title type="html">Responsible data considerations for sharing human rights data</title><link href="http://localhost:4000/2015/10/24/human-rights-data-sharing/" rel="alternate" type="text/html" title="Responsible data considerations for sharing human rights data" /><published>2018-02-27T00:00:00+00:00</published><updated>2018-02-27T00:00:00+00:00</updated><id>http://localhost:4000/2015/10/24/responsible-data-human-rights-data</id><content type="html" xml:base="http://localhost:4000/2015/10/24/human-rights-data-sharing/">&lt;p&gt;&lt;em&gt;This post was written by Tom Walker and Fieke Jansen, with input from Rachel Rank and Lorena Klos.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For many human rights funders, communicating clearly about the activities they fund (and why they are valuable) is an important part of their work. Indeed, funders often favour transparency on principle as an inherent good. How can they balance this with the responsibility to avoid harm, both to their grantees and the people their grantees they aim to help?&lt;/p&gt;

&lt;p&gt;With &lt;a href=&quot;http://www.ariadne-network.eu/&quot;&gt;Ariadne&lt;/a&gt; and &lt;a href=&quot;http://www.threesixtygiving.org/&quot;&gt;360Giving&lt;/a&gt;, &lt;a href=&quot;https://www.theengineroom.org/&quot;&gt;The Engine Room&lt;/a&gt; is starting a research project to look deeper into these issues, supported by &lt;a href=&quot;https://digitalimpact.org/2017-digital-impact-grants-cohort-selected/&quot;&gt;Digital Impact&lt;/a&gt; (part of the Digital Civil Society Lab at Stanford University).&lt;/p&gt;

&lt;p&gt;We’ve been interested in the topic since 2014, when we wrote a &lt;a href=&quot;https://www.theengineroom.org/wp-content/uploads/2016/07/Responsible-Data-Forum-Data-in-the-donor-community.pdf&quot;&gt;report based on interviews with 28 staff in 14 different grant-making organisations&lt;/a&gt;, and it’s time for an update. We’ll be talking to a wide range of people, with the aim of finding out what they need to know to make strong, effective decisions. Before getting started, we’ve been looking at what we already know about funders’ data-sharing practices through a series of blogposts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.theengineroom.org/sharing-data-responsibly-funders/&quot;&gt;The first looked at how and why funders’ are sharing grantee data publicly&lt;/a&gt; and with other funders. This post, the second of two, discusses how funders are publicly describing their approach to the responsible data implications of sharing grantee data, as well as steps they’re taking in response to those issues.&lt;/p&gt;

&lt;h3 id=&quot;challenges-to-sharing-data-responsibly&quot;&gt;Challenges to sharing data responsibly&lt;/h3&gt;

&lt;p&gt;The mapping exercise raised a number of challenges that funders seeking to share data face, including adapting to closing civic space, balancing promoting transparency with limiting harm to grantees and collecting informed consent.&lt;/p&gt;

&lt;h4 id=&quot;transparency-in-the-context-of-closing-civic-space&quot;&gt;Transparency in the context of closing civic space&lt;/h4&gt;

&lt;p&gt;In many countries, civil society organisations’ space to operate is diminishing rapidly. According to the International Center for Not-for-Profit Law (ICNL), in 2015-16 governments worldwide &lt;a href=&quot;http://www.icnl.org/research/trends/&quot;&gt;adopted 64 laws, regulations, and other initiatives&lt;/a&gt; to limit the operations of non-governmental organisations, including by restricting foreign funding or threatening to de-register them. Funding organisations with a prominent founder may also be subject to &lt;a href=&quot;https://www.channel4.com/news/factcheck/why-the-conspiracy-theories-about-george-soros-dont-stack-up&quot;&gt;public criticism&lt;/a&gt; on the basis of their grant-making decisions. As the Transparency and Accountability Initiative’s 2017 report &lt;a href=&quot;http://www.transparency-initiative.org/uncategorized/1996/distract-divide-detach-using-transparency-accountability-justify-regulation-csos/&quot;&gt;Distract, Divide and Detach&lt;/a&gt; explains, governments are increasingly attempting to discredit civil society organisations by focusing on any foreign connections they may have. Could publishing data demonstrating that an organisation has received money from a foreign funder thus increase the risks faced by a grantee?&lt;/p&gt;

&lt;p&gt;As well as targeting organisations legally, is there a risk that adversaries could use data shared in grants to surveil or physically target individuals or organisations? This mapping exercise found no documented cases where a grantee or the people it seeks to help were physically targeted as a direct result of data published by a funder. (If you know of any, &lt;a href=&quot;mailto:research@theengineroom.org&quot;&gt;let us know.&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Although this could be because there are no such cases, it could also indicate the difficulty of drawing a direct connection between a funder’s data and individual cases of harm. Nevertheless, &lt;a href=&quot;https://responsibledata.io/resources/handbook&quot;&gt;the human rights funding community has a responsibility to use, share and publish data in a way that avoids causing harm&lt;/a&gt;. In particular, even when data has been de-identified, it may be possible for adversaries to re-identify individuals or organisations by combining it with other datasets. How regularly are funders thinking about these issues?&lt;/p&gt;

&lt;h5 id=&quot;publishing-data-in-volatile-environments&quot;&gt;Publishing data in volatile environments&lt;/h5&gt;

&lt;p&gt;Political environments and attitudes to funders can also change rapidly, making it harder for organisations to accurately assess future risks. As the Oak Foundation’s Head of Communications puts it &lt;a href=&quot;https://www.hrfn.org/community-voices/a-hard-lesson-with-an-important-result-wording-audiences-and-grantee-consent/&quot;&gt;in a 2015 blogpost&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Words [from published grant data] interpreted as appropriate language one day can be interpreted as anti-development, aggressive, hostile, or illegal at a later date…and this is hard to predict.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In one example of this trend, in 2015 the Indian government &lt;a href=&quot;https://m.csmonitor.com/World/Asia-South-Central/2015/0504/India-crackdown-Ford-Foundation-latest-foreign-NGO-slapped-by-Delhi&quot;&gt;cancelled the registrations of almost 9,000 civil society organisations that had received foreign funding&lt;/a&gt;, as well as publicly placing funders such as the Ford Foundation on a ‘watch list’. In such changing contexts, grantee data considered to be innocuous when first published could become more sensitive, potentially increasing risks to grantees as a result.&lt;/p&gt;

&lt;h5 id=&quot;transparency-vs-do-no-harm&quot;&gt;Transparency vs do no harm?&lt;/h5&gt;

&lt;p&gt;According to &lt;a href=&quot;http://simlab.org/resources/dogooddata/&quot;&gt;research by SIMLab&lt;/a&gt; (supported by a previous Digital Impact grant), some grantees are concerned about pressure from funders to share data:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“One former implementer told us that the pressure to provide funders with granular data for accountability, decision-making or just ‘openness’ was overwhelming, ‘but it’s hard to make sure it won’t get leaked, hacked or casually shared.’”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This also highlights a potential tension between funders’ desire for transparency and openness – described in &lt;a href=&quot;https://www.theengineroom.org/sharing-data-responsibly-funders/&quot;&gt;our first blogpost&lt;/a&gt; – and the principle of ‘do no harm.’ The report highlighted that the responsible data principle of ‘data minimisation’, which involves &lt;a href=&quot;https://medium.com/@Simon_B_Johnson/minimum-viable-data-when-less-data-is-more-valuable-d225b5fa9c1b&quot;&gt;only collecting data that is needed for a specific use case&lt;/a&gt;, “is not yet culturally ingrained in all stakeholders in the RD system.”&lt;/p&gt;

&lt;p&gt;This tension is also visible in rhetoric around data-sharing. In 2015, the IATI secretariat complimented USAID’s approach to publishing data, &lt;a href=&quot;https://www.aidtransparency.net/news/usaids-strategic-approach-to-improving-iati-compliance&quot;&gt;noting that&lt;/a&gt; it would “result in USAID publishing…the maximum level of compliance possible to continue to adhere to privacy laws.” Is there a risk that a focus on legal compliance could take precedence over a considered assessment of risks? We hope to look into this question in more detail during in the project.&lt;/p&gt;

&lt;p&gt;Writing in 2015, &lt;a href=&quot;http://carnegieendowment.org/2015/11/02/closing-space-challenge-how-are-funders-responding/ikrg?mkt_tok=3RkMMJWWfF9wsRohvKvKZKXonjHpfsX57O0oXqKg38431UFwdcjKPmjr1YQETMV0aPyQAgobGp5I5FEIQ7XYTLB2t60MWA%3D%3D&quot;&gt;Thomas Carothers suggested&lt;/a&gt; that funders may be “gravitating toward a kind of ‘transparency lite’ approach — being quite transparent about the specifics of their programming in contexts where they are not facing closing space, but selectively reducing available programme information in restrictive environments.” This project will aim to find out if this trend has continued, and any implications that this may have for funders’ approaches. As mentioned above, even a country that has space for civil society today can become a closed or contested space tomorrow.&lt;/p&gt;

&lt;h5 id=&quot;consent&quot;&gt;Consent&lt;/h5&gt;

&lt;p&gt;This mapping exercise found only limited publicly available information about funders’ processes when asking grantees for consent to share or publish their data. This raises a number of questions about how consent is implemented in practice. When and how are grantees informed on the potential sharing of application and organisation data? As discussed at a &lt;a href=&quot;https://www.hrfn.org/community-voices/responsible-data-forum-for-human-rights-defenders-key-takeaways/&quot;&gt;Human Rights Funders Network event in 2016&lt;/a&gt;, can grantees opt out of sharing information without negative consequences? As Danna Ingleton and Keith Armstrong, reporting back from the event, summarised the concern:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Will a prospective grantee view a question marked “optional” on a grant application as genuinely optional, or will they feel compelled to answer it because they think it will improve their chances of receiving the grant?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Separately, there is also a question of responsibility Who is responsible for ensuring that personal data is not published without consent: the individual funding organisations or the platforms that host the data?&lt;/p&gt;

&lt;p&gt;Several funders have publicly described how they have set up processes to address this: for example, the Oak Foundation states that before grant descriptions are published publicly, they are reviewed and approved by the partner themselves. This process was set up partly in response to a 2011 incident in which Oak mistakenly &lt;a href=&quot;https://www.hrfn.org/community-voices/a-hard-lesson-with-an-important-result-wording-audiences-and-grantee-consent/&quot;&gt;published descriptions of grants that staff used internally (rather than the edited versions included in annual reports)&lt;/a&gt; as part of their grants database, provoking criticism from a blogger that the descriptions were “ambiguous and inflammatory.”&lt;/p&gt;

&lt;p&gt;360Giving provides &lt;a href=&quot;http://standard.threesixtygiving.org/en/latest/data-protection/&quot;&gt;guidelines&lt;/a&gt; for asking grantees’ consent to publish data, and examples of how to present such requests. We’re interested in hearing about any other examples, and looking forward to finding out more about how other organisations’ processes work.&lt;/p&gt;

&lt;h5 id=&quot;how-can-funders-and-grantees-assess-the-risks-of-sharing-data&quot;&gt;How can funders and grantees assess the risks of sharing data?&lt;/h5&gt;

&lt;p&gt;In &lt;a href=&quot;https://www.hrfn.org/community-voices/responsible-data-forum-for-human-rights-defenders-key-takeaways/&quot;&gt;the Human Rights Funders Network’s 2016 event&lt;/a&gt; on responsible data practice, discussion focused on the need for grant-makers to evaluate the risks of sharing data more accurately. On the basis of this mapping exercise, it is unclear if judgements about what data is ‘sensitive’ are usually made by individual programme officers or programme heads in funding organisations: during this mapping exercise, we found several instances where this was the case. Does this risk relying too much on the accuracy of an individual’s interpretation of a grantee’s situation?&lt;/p&gt;

&lt;h5 id=&quot;trade-offs-publishing-funding-data-responsibly-may-limit-its-usefulness-for-analysis&quot;&gt;Trade-offs: Publishing funding data responsibly may limit its usefulness for analysis&lt;/h5&gt;

&lt;p&gt;As the UK Data Service (which, among other things, archives data collected by civil society organisations) points out, removing some data may make overall datasets less comprehensive, and thus &lt;a href=&quot;https://www.ukdataservice.ac.uk/media/604168/hrdw_bishop29102015.pdf&quot;&gt;less useful for analysis&lt;/a&gt;. For example, although a dataset might indicate that human rights groups working on a specific topic are underfunded, in reality several funders may have intentionally excluded them from a dataset because of responsible data considerations. Initiatives collecting and aggregating data in this way often deal with this by explicitly stating their methodology and highlighting limitations in the data sample. We’re looking forward to hearing if other organisations are thinking about these issues, too.&lt;/p&gt;

&lt;h3 id=&quot;how-are-funders-trying-to-share-data-responsibly&quot;&gt;How are funders trying to share data responsibly?&lt;/h3&gt;

&lt;p&gt;Human rights funders need to balance a number of issues when sharing data, including mitigating potential harm to an individual or grantee, safeguarding the reliability of the data when published, and promoting the sharing of data whenever possible. This mapping found that indications that funders are thinking about these issues in a number of ways:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some funders may be collecting less data from grantees&lt;/strong&gt;. On the basis of interviews conducted in 2015, the &lt;a href=&quot;http://carnegieendowment.org/2015/11/02/closing-space-challenge-how-are-funders-responding/&quot;&gt;Carnegie Endowment suggested&lt;/a&gt; that “some funders are discussing internally whether they should…reduce [grantees’] vulnerability to monitoring by hostile security services, by collecting less detailed information about them [and] asking grantees to report in less detail about what they do, and conducting fewer site visits to grantee organisations.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other funders may be publishing or sharing less of the data they collect&lt;/strong&gt;. Funders may decide not to publish or redact grantee data by stripping out specific data fields (such as grantee addresses) or choosing not to publish selected elements of data. For an example of this approach, the Advancing Human Rights Initiative &lt;a href=&quot;http://humanrightsfunding.org/about/?tab=what-challenges-did-we-encounter&quot;&gt;asks funders to list grantees working on sensitive projects as “anonymous,”&lt;/a&gt; and requests that they only disclose information that would not compromise the grantee’s safety. Several funders chose to submit all grants data anonymously. Notably, the initiative also said that it does not publicly share aggregate data (such as country-level trends), perhaps because of concerns about the uses other actors could make data in this format.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some groups create (or follow) guidelines for publishing grantee data responsibly&lt;/strong&gt;. Some organisations are responding by &lt;a href=&quot;https://responsibledata.io/2015/04/30/report-back-closed-roundtable-on-organizational-responsible-data-policies/&quot;&gt;creating policies or guidelines&lt;/a&gt;. For example, 360Giving’s guidelines to funders publishing data advise that: “There can be cases where grants data is sensitive for reasons other than privacy. For example, the address of a women’s refuge might be inappropriate to include in data about a grant to that organisation.” In a small number of cases, the funders themselves provide guidelines: the UK-based Big Lottery Fund gives grantees &lt;a href=&quot;https://www.biglotteryfund.org.uk/funding/funding-guidance/managing-your-funding/about-equalities/more-information/data-protection&quot;&gt;advice on how to submit data to them&lt;/a&gt; in line with national data protection legislation. The Digital Impact Toolkit hosts a &lt;a href=&quot;https://digitalimpact.io/document/data-sharing-funder-required-plan/&quot;&gt;draft data sharing policy&lt;/a&gt;, while the Responsible Data community’s Handbook includes a &lt;a href=&quot;https://the-engine-room.github.io/responsible-data-handbook/chapters/chapter-02c-sharing-data.html#sharing-data&quot;&gt;section on sharing data responsibly&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To take another example, organisations publishing to the IATI Standard that want to exclude some data are required to state their reasons for doing so in a public exclusion policy. &lt;a href=&quot;http://iatistandard.org/202/guidance/how-to-publish/establish-publishing-policies/#exclusions&quot;&gt;In its guidance&lt;/a&gt;, IATI states that funders may not wish to publish details of their work and the areas in which it takes place “if your organisation carries out works which could be considered by states or societies as illegal or unacceptable,” or “if you are working in sensitive geographic areas.” Funders listed in the IATI registry adopt different approaches to this. Some publish exclusion policies stating that, in general terms, they will not publish data for programmes working on issues that are considered sensitive; others say that they will exclude specific types of information, while still others include no explicit exclusion policies.&lt;/p&gt;</content><author><name>Tom Walker</name></author><category term="technology, research" /><summary type="html">This post was written by Tom Walker and Fieke Jansen, with input from Rachel Rank and Lorena Klos.</summary></entry><entry><title type="html">How small data can improve access to justice for the poor</title><link href="http://localhost:4000/2018/02/10/access-to-justice-tech/" rel="alternate" type="text/html" title="How small data can improve access to justice for the poor" /><published>2018-02-10T00:00:00+00:00</published><updated>2018-02-10T00:00:00+00:00</updated><id>http://localhost:4000/2018/02/10/technology-access-to-justice</id><content type="html" xml:base="http://localhost:4000/2018/02/10/access-to-justice-tech/">&lt;p&gt;&lt;em&gt;This post originally appeared on &lt;a href=&quot;https://www.opensocietyfoundations.org/voices/how-small-data-can-improve-access-justice-poor&quot;&gt;Open Society Foundations’ blog, Voices&lt;/a&gt;. The Engine Room is a grantee of the Open Society Foundations.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A tenant becomes homeless because she or he doesn’t understand how to challenge a landlord’s eviction order. A farmer lacks the ability to prove the legal ownership of traditional lands. A young woman can’t get access to state sponsored medical care because she lacks the right documentation.&lt;/p&gt;

&lt;p&gt;These are all examples of how barriers to justice are both a cause and a consequence of poverty and inequality. Without meaningful access to justice, people are more likely to face problems with health, employment, housing, and education.&lt;/p&gt;

&lt;p&gt;These issues are at the heart of efforts by the Open Society Foundations and other partners to push the world’s governments to deliver on the promise made in the &lt;a href=&quot;https://www.un.org/sustainabledevelopment/peace-justice/&quot;&gt;Sustainable Development Goals&lt;/a&gt; to deliver “equal access to justice for all.” But what does that look like? How do we measure it? And how do we know what works, and what doesn’t?&lt;/p&gt;

&lt;p&gt;Thanks to the more than 50 large-scale legal needs surveys which have been conducted in over 30 countries, we now know more than ever about the prevalence of various legal problems, global inequalities in people’s experience of the law, common barriers to justice, and the social and economic costs of ineffective access to justice. Most recently, the World Justice Project’s 2018 Global Insights on Access to Justice provided comparable data on legal needs and access to justice on a global scale. Identifying the need is one thing. But how do we create and sustain effective access to justice solutions?&lt;/p&gt;

&lt;p&gt;For one thing, we need more and better “small data” from within justice systems. Small data is drawn from the experience of the user and offers an accessible way to understand and address specific problems. It includes, for example, the individual case data gathered by organizations serving poor and marginalized communities, which draws on the problems faced by real people.&lt;/p&gt;

&lt;p&gt;When collected and analyzed, this data can help identify the problems that matter to a community. It can shape the best ways to address those problems, and it can help us assess the actual impact of a particular solution. Small data can also give us the information to persuade governments and private funders to invest in the provision of community-based justice systems by demonstrating how a low-cost solution can save money—and, sometimes, prevent a costly crisis.&lt;/p&gt;

&lt;p&gt;While it does not replace large-scale legal needs studies, collecting this data is less expensive because it is generated through the day-to-day provision of legal services. It also adds an important complementary picture; it reflects experiences navigating real-time legal processes, whereas surveys often rely on people’s recall or perceptions.&lt;/p&gt;

&lt;p&gt;To explore this potential, the Open Society Justice Initiative and several of the Open Society national foundations teamed up with &lt;a href=&quot;https://www.theengineroom.org/&quot;&gt;The Engine Room&lt;/a&gt;, an international organization that helps nonprofits make the most of data and technology, to research how organizations collect, manage, analyze, and share data on community-based justice provisions.&lt;/p&gt;

&lt;p&gt;We also explored the role that case management technology plays in helping organizations to improve the quality of their advocacy and provide services to larger numbers of people in more effective and efficient ways. Our &lt;a href=&quot;http://www.theengineroom.org/wp-content/uploads/2018/04/Technology-in-Case-Management-for-Legal-Empowerment-The-Engine-Room.pdf&quot;&gt;report explores the use and impact of technology&lt;/a&gt; in case management in Indonesia, Moldova, Mongolia, Sierra Leone, and South Africa, informed by interviews with a range of civil society organizations, technologists, and government representatives.&lt;/p&gt;

&lt;p&gt;Here’s what we learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Frontline case data can help to identify, in a compelling way, problems that need fixing based on local priorities&lt;/strong&gt;. In 2012 and 2013, for example, the South African organization Black Sash used case data from local &lt;a href=&quot;https://www.opensocietyfoundations.org/sites/default/files/community-legal-southafrica-20160920.pdf&quot;&gt;Community Advice Offices&lt;/a&gt; to discover that money was systematically—and inappropriately—deducated from people’s government benefit payments. Using data from more than 120 cases, from across three provinces, Black Sash ran a campaign that successfully ended the practice.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collecting more data on legal aid and community-based justice can demonstrate the scale and impact of access to justice interventions&lt;/strong&gt;. For example, the &lt;a href=&quot;https://www.opensocietyfoundations.org/sites/default/files/community-legal-moldova-20160920.pdf&quot;&gt;National Legal Aid Council in Moldova&lt;/a&gt; recently introduced a system that will collect data on paralegals’ activities nationwide, helping to build the case for sustainable, long-term support. It can also generate insights into complex issues by sharing data between organizations, as demonstrated by those organizations in the United Kingdom who collaborated to investigate &lt;a href=&quot;http://www.datakind.org/projects/sharing-data-to-learn-about-homelessness&quot;&gt;common advice needs among homeless people&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Case data can demonstrate that community-based justice reduces public spending,&lt;/strong&gt; &lt;a href=&quot;https://www.accesstojusticeactiongroup.co.uk/wp-content/uploads/2011/07/towards_a_business_case_for_legal_aid.pdf&quot;&gt;by limiting the use of state resources&lt;/a&gt;. For example, a 2007–2010 analysis of 338 cases in Indonesia showed that paralegals in Indonesia often found alternative solutions that &lt;a href=&quot;https://namati.org/resources/paralegalism-and-legal-aid-in-indonesia-2011/&quot;&gt;minimized the need to involve police&lt;/a&gt;, mediating between conflicting parties in 54 percent of cases reviewed. The Open Society Foundations are currently working with organizations in South Africa and Sierra Leone to conduct further research into the economic benefits to government, and to make the case for expanded justice services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Well-designed technology systems for collecting case data can make legal empowerment work more efficiently&lt;/strong&gt;, reducing costs and time spent on administration. By replacing manual processes with a new case management system, &lt;a href=&quot;http://hotdocs.com/solutions/client-stories/legal-aid-south-africa&quot;&gt;Legal Aid South Africa reduced the number of managers required to process cases from 64 to five&lt;/a&gt;. Indonesia’s Ministry of Law and Human Rights has said that moving to an online system has doubled the amount of money successfully reimbursed to independent legal aid organizations nationwide.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collecting data effectively and regularly can also make organizations more responsive&lt;/strong&gt;. Organizations we spoke to in Sierra Leone were starting to use case data both to identify paralegals’ training needs, and to shed light on questions such as the proportion of people who access local courts rather than the formal justice system.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our research suggests that harnessing the benefits of case management systems takes time and resources; careful attention to context; and thoughtful, sustained engagement on the needs of users. But it also highlights how “small data” can have a big impact. By bringing together experiences from a wide range of national contexts to identify useful strategies and approaches, the Open Society Foundations are aiming to contribute to this process.&lt;/p&gt;</content><author><name>Tom Walker</name></author><category term="technology, research" /><summary type="html">This post originally appeared on Open Society Foundations’ blog, Voices. The Engine Room is a grantee of the Open Society Foundations.</summary></entry><entry><title type="html">How can we make research used and useful?</title><link href="http://localhost:4000/2017/10/24/xxx/" rel="alternate" type="text/html" title="How can we make research used and useful?" /><published>2017-11-22T00:00:00+00:00</published><updated>2017-11-22T00:00:00+00:00</updated><id>http://localhost:4000/2017/10/24/mavc-communicating-research</id><content type="html" xml:base="http://localhost:4000/2017/10/24/xxx/">&lt;p&gt;When we summarised findings from Making All Voices Count’s research on how civil society organisations design and use technology, we found that they often struggle to set realistic expectations for their tech projects, do the right amount of user research, connect with the people they want to reach and adapt to unexpected changes (Read more on the programme’s findings on other themes, like &lt;a href=&quot;http://www.makingallvoicescount.org/blog/whats-needed-get-state-respond-citizens-part-problem/&quot;&gt;government responsiveness&lt;/a&gt; and &lt;a href=&quot;http://www.makingallvoicescount.org/publication/&quot;&gt;public participation&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;We wanted to try setting these messages out in a way that made them useful and easy to read. So we developed a microsite (a standalone, single-page website that displays a specialised set of content) to summarise the messages, and point readers directly to the evidence behind them: &lt;a href=&quot;https://researchfindings.tech/&quot;&gt;https://researchfindings.tech/&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;why-a-microsite&quot;&gt;Why a microsite?&lt;/h4&gt;

&lt;p&gt;Researchers have been asking &lt;a href=&quot;http://www.makingallvoicescount.org/blog/research-evidence-ethics-civic-tech-call-action/&quot;&gt;why existing research is not being used&lt;/a&gt; for some time now. Recently, there have been a series of calls to &lt;a href=&quot;https://methodicalsnark.org/2017/09/28/designing-civic-tech-research-at-scale-to-be-useful-why-methods-matter/&quot;&gt;understand what kind of evidence practitioners are actually likely to use&lt;/a&gt;, and to &lt;a href=&quot;https://medium.com/@laurawmcd/the-evidence-agenda-appealing-for-rationality-in-tech-for-social-change-41de18e4805a&quot;&gt;deal with the tech-for-social-change sector’s ‘weak culture of evidence and accountability’&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These are key parts of the picture. But we think it’s also important to think about how that evidence is presented. When practitioners look for evidence, it may be hard for them to quickly work out what will be practically useful. The findings are there - they’re just mixed up with large blocks of text, or written in a style the practitioner isn’t used to.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We really need a focus on joining up lessons and making them digestible when designing anything that looks at technology and transparency in governance. - Amy O’Donnell, Oxfam GB&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Maybe it’s about the format of the evidence itself – rather than expecting busy practitioners to read long blocks of text, what about asking them to listen to experts talking about the research in their own words? In many places around the world, radio is the main way that people get information. Though obviously we couldn’t tap into radio stations or those networks, we wanted to try out providing research information in digestible soundbites.&lt;/p&gt;

&lt;p&gt;So, at Making All Voices Count’s final learning event “Appropriating technology for accountability’, we interviewed people from various countries and types of organisation: Michael (Miko) Canares from the Web Foundation’s &lt;a href=&quot;http://labs.webfoundation.org/&quot;&gt;Open Data Labs&lt;/a&gt;, Amy O’Donnell from &lt;a href=&quot;https://policy-practice.oxfam.org.uk/our-people/programme-implementation/amy-odonnell&quot;&gt;Oxfam GB&lt;/a&gt;, Koketso Moeti from &lt;a href=&quot;http://amandla.mobi/&quot;&gt;amandla.mobi&lt;/a&gt; in South Africa and the &lt;a href=&quot;https://lindaraftree.com/&quot;&gt;independent consultant&lt;/a&gt; Linda Raftree. Listen to the audio here:&lt;/p&gt;

&lt;h5 id=&quot;testing-how-practitioners-read-research-online&quot;&gt;Testing how practitioners read research online&lt;/h5&gt;

&lt;p&gt;Earlier in 2017, we at &lt;a href=&quot;https://www.theengineroom.org/&quot;&gt;The Engine Room&lt;/a&gt; had put this idea to the test by asking practitioners to read and engage with research we’d written ourselves.&lt;/p&gt;

&lt;p&gt;At the time, we were working on &lt;a href=&quot;https://alidade.tech/&quot;&gt;Alidade&lt;/a&gt;: an interactive tool that walks you through the process of choosing a technology tool, highlighting relevant resources and research findings along the way. We decided to run online walkthroughs with 10 project managers working on transparency and accountability initiatives in countries ranging from Nigeria to Indonesia.&lt;/p&gt;

&lt;p&gt;We asked them to read a one-page, online summary of the research, while sharing their screens with us so that we could see how they browsed. Our hopes were high: all of them had said they were interested in reading evidence about their work, and we’d tried to make the page as short as we could, split findings up into bullet points and highlighted key passages.&lt;/p&gt;

&lt;p&gt;Perhaps unsurprisingly, we were disappointed. Most only skimmed the research findings. Those with a stronger interest in research said that their eyes were drawn to the most skimmable parts: text in larger type, and the few graphics we’d included - even when they weren’t the most accurate reflection of the most important messages. Like all of us, they were dealing with an online information glut by trying to quickly assess if any of the information was relevant - and ignoring the rest.&lt;/p&gt;

&lt;p&gt;These observations won’t be news to anyone familiar with &lt;a href=&quot;https://www.newyorker.com/science/maria-konnikova/being-a-better-online-reader&quot;&gt;research into how we read online&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;People will often say that we don’t know where to find existing research. In a space like ours you’re exposed to so much of it, but you don’t make use of it. - Koketso Moeti&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They also fit with the broader trend that we identified throughout Making All Voices Count’s research on the design of technology projects: that organisations weren’t doing enough research into their users’ needs. We asked Koketso, Linda, Miko and Amy why.&lt;/p&gt;

&lt;p&gt;So, the microsite we produced aims to highlight five simple messages, the most important evidence backing this up, and some ideas on how to do things differently. It’s designed to only show information that the reader is specifically interested in, reducing the amount of information to a minimum. Is this format a useful one for presenting evidence? We’ve been encouraged by some of the feedback we’ve already had:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://twitter.com/ppolitics&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/740900955055640576/dMbl45tk_normal.jpg&quot; alt=&quot;Image&quot; /&gt;&lt;/a&gt; I like the accessible info display of &lt;a href=&quot;https://twitter.com/EngnRoom&quot;&gt;@EngnRoom&lt;/a&gt; findings on tech &amp;amp; international development NGO’s: &lt;a href=&quot;https://t.co/r4rygssGcP&quot;&gt;https://researchfindings.tech/ &lt;/a&gt; - via &lt;a href=&quot;https://twitter.com/Civicist&quot;&gt;@Civicist&lt;/a&gt;. &lt;a href=&quot;https://twitter.com/ppolitics/status/923551092205637632&quot;&gt;2:05 PM - Oct 26, 2017&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://twitter.com/RChandran1&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/699427930150084608/OofjpErs_normal.jpg&quot; alt=&quot;Image&quot; /&gt;&lt;/a&gt; Great stuff &lt;a href=&quot;https://twitter.com/hashtag/AllVoicesCount?src=hash&quot;&gt;#AllVoicesCount&lt;/a&gt;:tech+civil society=a struggle. Love simple, clear “where’s the evidence?” &amp;amp; “so what?” &lt;a href=&quot;https://t.co/4fZIJ8OvXu&quot;&gt;https://researchfindings.tech/ &lt;/a&gt; &lt;a href=&quot;https://twitter.com/RChandran1/status/923567833619058688&quot;&gt;3:11 PM - Oct 26, 2017&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, we’re also keen to hear more views - is this a useful way of sharing evidence? What could we do differently?&lt;/p&gt;</content><author><name>Tom Walker</name></author><category term="technology, research" /><summary type="html">When we summarised findings from Making All Voices Count’s research on how civil society organisations design and use technology, we found that they often struggle to set realistic expectations for their tech projects, do the right amount of user research, connect with the people they want to reach and adapt to unexpected changes (Read more on the programme’s findings on other themes, like government responsiveness and public participation).</summary></entry><entry><title type="html">How to make sure that tech projects meet the brief</title><link href="http://localhost:4000/2017/10/22/xxx/" rel="alternate" type="text/html" title="How to make sure that tech projects meet the brief" /><published>2017-10-22T00:00:00+01:00</published><updated>2017-10-22T00:00:00+01:00</updated><id>http://localhost:4000/2017/10/22/mavc-research-review</id><content type="html" xml:base="http://localhost:4000/2017/10/22/xxx/">&lt;p&gt;Over the last four years, Making All Voices Count (MAVC) has published about 70 research reports, practice papers and journal articles, investigating tech-enabled projects that aim to amplify citizens’ voices and encourage government to respond to them.&lt;/p&gt;

&lt;p&gt;They cover a huge range of kinds of organisation and types of technology, ranging from broad assessments of 38 organisations’ tech selection processes to in-depth accounts of how one Ghanaian organisation implemented an interactive voice response survey.&lt;/p&gt;

&lt;p&gt;This research holds huge potential for practitioners working in this space. So, as the programme draws to a close, my colleagues at The Engine Room and I have been reading through these research pieces and looking for common threads.&lt;/p&gt;

&lt;h4 id=&quot;making-tech-effective--building-on-what-we-already-know&quot;&gt;Making tech effective – building on what we already know&lt;/h4&gt;

&lt;p&gt;One thread stands out: the way that organisations choose and use technology, and how this affects their projects. The overall picture painted by the research is clear: many organisations are struggling to use technology in a way that they feel makes their projects more effective.&lt;/p&gt;

&lt;p&gt;This may come as no surprise. (&lt;a href=&quot;http://www.makingallvoicescount.org/blog/research-evidence-ethics-civic-tech-call-action/&quot;&gt;Researchers have been saying similar things&lt;/a&gt; for some time now.) The factors behind it are not entirely new, either: earlier research has repeatedly cited the need for &lt;a href=&quot;https://www.ids.ac.uk/publication/learning-study-on-the-users-in-technology-for-transparency-and-accountability-initiatives-assumptions-and-realities&quot;&gt;stronger understandings of what users need&lt;/a&gt; and &lt;a href=&quot;http://www.transparency-initiative.org/uncategorized/513/global-mapping-of-technology-for-transparency-and-accountability/&quot;&gt;deeper collaboration&lt;/a&gt; with a wider range of groups that projects want to influence.&lt;/p&gt;

&lt;p&gt;But, taken together, Making All Voices Count’s research outputs add considerable weight to the evidence behind these messages – and should make them even harder to ignore. What’s more, they point to practical steps that projects are already taking to address these challenges.&lt;/p&gt;

&lt;h4 id=&quot;thoughts-on-when-tech-works-from-the-horses-mouth&quot;&gt;Thoughts on when tech works, from the horse’s mouth&lt;/h4&gt;

&lt;p&gt;Many of the research pieces concentrate on asking organisations how they felt their project had gone, and what role tech played.&lt;/p&gt;

&lt;p&gt;This is significant partly because it’s relatively rare. In a sector that’s often accused of trying to run before it can walk, projects don’t often get the chance to step back and reflect – especially in a way that researchers can compare across multiple projects. Making All Voices Count actively encouraged this reflection throughout the programme, and will carry on doing so at its final learning event this week.&lt;/p&gt;

&lt;p&gt;But this research method was also interesting for another reason: it may have encouraged organisations to be candid.&lt;/p&gt;

&lt;p&gt;Researchers often asked organisations to describe their project’s progress from beginning to end, or think back to a specific moment that was important for their project. Many participants were given anonymity, or the space to explain their project’s context in detail, and as a result, had more freedom to describe how they had really used tech.&lt;/p&gt;

&lt;p&gt;Many responded by being, as one report put it, “unexpectedly open and often self-critical” about their own work. As such, the research frequently gives valuable insights into the unvarnished reality of designing and implementing technology in transparency and accountability initiatives.&lt;/p&gt;

&lt;h4 id=&quot;many-organisations-are-disillusioned-with-the-contribution-technology-had-made-to-their-project&quot;&gt;Many organisations are disillusioned with the contribution technology had made to their project&lt;/h4&gt;

&lt;p&gt;As we read, we repeatedly encountered one overriding sentiment: disappointment. All too often, organisations told researchers that they were dissatisfied with what tech had been able to do for them.&lt;/p&gt;

&lt;p&gt;For example, of the seven MAVC-funded projects that &lt;a href=&quot;http://www.makingallvoicescount.org/publication/ict-facilitated-accountability-engagement-health-systems-review-making-voices-count-mhealth-accountability-projects/&quot;&gt;used information and communication technologies (ICTs) to promote accountability in health systems&lt;/a&gt;, four experienced: “disconnects between their expectations of what the technology could do, what it actually did, and the implications for accountability.” Meanwhile, in Kenya and South Africa, &lt;a href=&quot;https://alidade.tech/page/research&quot;&gt;more than 75% of the 38 transparency and accountability initiatives&lt;/a&gt; interviewed said that they weren’t happy with the technology tools they’d chosen. Many had already moved on to a new tool by the time that the researchers spoke to them.&lt;/p&gt;

&lt;p&gt;In several cases, researchers linked this to organisations’ limited knowledge of the people they wanted to use the tools. Only 15 of the 38 Kenyan and South African initiatives &lt;a href=&quot;https://alidade.tech/page/research&quot;&gt;did any user research before choosing a technology tool&lt;/a&gt;. More than half of the total then said that their tool had not been used in the way they had hoped.&lt;/p&gt;

&lt;p&gt;Elsewhere, it was attributed to more technical issues. In Kenya, &lt;a href=&quot;http://www.makingallvoicescount.org/publication/digital-development-differently-lessons-adaptive-management-technology-governance-initiatives-kenya/&quot;&gt;59% of the 24 projects interviewed by researchers&lt;/a&gt; said that a lack of technical knowledge was a barrier for their projects: a finding echoed by &lt;a href=&quot;http://www.makingallvoicescount.org/publication/ict-facilitated-accountability-engagement-health-systems-review-making-voices-count-mhealth-accountability-projects/&quot;&gt;Hyrnick and Waldman’s mHealth research&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;but-we-now-have-more-evidence-on-whats-helping-organisations-make-better-decisions&quot;&gt;But we now have more evidence on what’s helping organisations make better decisions&lt;/h4&gt;

&lt;p&gt;When user research did take place, in-depth practice papers showed the profound difference that it made to projects. For example, the &lt;a href=&quot;http://www.makingallvoicescount.org/publication/lessons-yowzits-pracitioner-research-learning-process/&quot;&gt;South African organisation Yowzit used user research to discover that its users would only find their citizen participation platform convenient&lt;/a&gt; if they spoke English, were digitally literate and already used similar ratings platforms. This helped them to target their work with a revised, tighter focus.&lt;/p&gt;

&lt;p&gt;Researchers found that although organisations had to invest time up-front in doing thorough user research, it could actually save them effort in the long run. In South Africa, for example, &lt;a href=&quot;http://www.makingallvoicescount.org/publication/giving-voice-clients-post-rape-services-building-piloting-feedback-mechanism-tshwane/&quot;&gt;the Foundation for Professional Development (FPD)’s user research&lt;/a&gt; showed them that they needed to focus on building an app focused on clients’ needs, rather than the more ambitious programme they had planned. Others found effective ways of combining offline and online content, as with &lt;a href=&quot;http://www.makingallvoicescount.org/publication/open-mapping-ground-learning-map-kibera/&quot;&gt;Map Kibera’s community engagement work&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The research re-emphasised that projects that use tech depend on strong relationships as well as well-designed technology products. For example, Tiago Peixoto and Jonathan Fox found, projects using technology to increase government responsiveness are &lt;a href=&quot;http://www.makingallvoicescount.org/publication/ict-enabled-citizen-voice-lead-government-responsiveness/&quot;&gt;more likely to suceed when governments already want to get (and act on) feedback&lt;/a&gt;from citizens. Meanwhile, the multi-country assessment of mHealth initiatives suggested that &lt;a href=&quot;http://www.makingallvoicescount.org/publication/ict-facilitated-accountability-engagement-health-systems-review-making-voices-count-mhealth-accountability-projects/&quot;&gt;projects were more likely to report success when they had strong, long-standing connections&lt;/a&gt; with the the people or government actors they were working with.&lt;/p&gt;

&lt;h4 id=&quot;changing-course-during-a-project-the-toughest-challenge-of-all&quot;&gt;Changing course during a project: the toughest challenge of all&lt;/h4&gt;

&lt;p&gt;Finally, research indicated that organisations need to be able to do more than this. They also needed to be able to research &lt;a href=&quot;http://www.saiia.org.za/policy-briefings/why-isn-t-tech-for-accountability-working-in-africa&quot;&gt;all three aspects required for a successful tech project&lt;/a&gt;: the accountability problem they were trying to solve; the people they wanted to reach; and the tech options available. Then, they needed to put what they learned into practice, by adapting to what they found out during the process. In Kenya and South Africa, &lt;a href=&quot;https://alidade.tech/page/research&quot;&gt;those of the 38 organisations that trialled their tools with their users&lt;/a&gt; were by far the most likely to be happy with their eventual choice.&lt;/p&gt;

&lt;p&gt;This challenge proved to be beyond many of the organisations that took part in the research. Looking back, &lt;a href=&quot;http://www.makingallvoicescount.org/publication/digital-development-differently-lessons-adaptive-management-technology-governance-initiatives-kenya/&quot;&gt;79% of the 24 Kenyan organisations interviewed said that “adapting to context mattered a lot for the project&lt;/a&gt;,” but were unable to adapt fully themselves. For projects to be genuinely adaptive, they need to be supported with flexible funding and tools for transparent communication with funders. The &lt;a href=&quot;http://www.makingallvoicescount.org/publication/giving-voice-clients-post-rape-services-building-piloting-feedback-mechanism-tshwane/&quot;&gt;FPD&lt;/a&gt; and &lt;a href=&quot;http://www.makingallvoicescount.org/publication/lessons-yowzits-pracitioner-research-learning-process/&quot;&gt;Yowzit&lt;/a&gt; practice papers provide examples of this approach in action, and we’re looking forward to discussing what this means&lt;/p&gt;

&lt;h4 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h4&gt;

&lt;p&gt;The research that’s come out of this four-year programme is a valuable contribution to our understanding of how tech projects working in transparency and accountability can be most effective. Being given time to reflect and critically assess your own work, doesn’t come around that often, and we’re grateful to Making All Voices Count for providing many such opportunities.&lt;/p&gt;

&lt;p&gt;If the research findings are taken as practical lessons that influence future work by practitioners and civil society more broadly, they will have a better chance of success. The next challenges are for practitioners to understand which findings could influence the way they design their next project, and for funders and intermediaries to keep supporting the kind of networks that allow these findings to spread widely.&lt;/p&gt;</content><author><name>Tom Walker</name></author><category term="technology, research" /><summary type="html">Over the last four years, Making All Voices Count (MAVC) has published about 70 research reports, practice papers and journal articles, investigating tech-enabled projects that aim to amplify citizens’ voices and encourage government to respond to them.</summary></entry><entry><title type="html">Improving how we measure impact - a round-up of projects and approaches</title><link href="http://localhost:4000/2015/10/24/measuring-impact/" rel="alternate" type="text/html" title="Improving how we measure impact - a round-up of projects and approaches" /><published>2014-10-11T00:00:00+01:00</published><updated>2014-10-11T00:00:00+01:00</updated><id>http://localhost:4000/2015/10/24/improving-impact-measurement</id><content type="html" xml:base="http://localhost:4000/2015/10/24/measuring-impact/">&lt;p&gt;For the last couple of years, the engine room has been exploring how to measure whether technology for transparency and accountability (tech-for-T&amp;amp;A) projects actually have an impact. It’s an area filled with lengthy evaluation reports and endless discussions on methodologies, but few solid conclusions. Dip even a cautious toe into these murky waters and you’ll find a researcher complaining that the evidence available is ‘&lt;a href=&quot;http://www.transparency-initiative.org/reports/impact-case-studies-from-middle-income-and-developing-countries.&quot;&gt;weak&lt;/a&gt;’ or ‘&lt;a href=&quot;http://www.thebrokeronline.eu/Articles/Apparently-transparent&quot;&gt;uneven, piecemeal and scattered&lt;/a&gt;‘.&lt;/p&gt;

&lt;p&gt;So, why persevere? As it turns out, initiatives that are set up to capture the messy reality of what goes on during a project are often in a better position to build on their gains and learn from setbacks – and impress donors at the same time. The engine room recently published a &lt;a href=&quot;https://www.theengineroom.org/diy-for-me/&quot;&gt;hands-on guide&lt;/a&gt; for programme staff on this very topic, setting out 17 concrete steps for monitoring projects on the go.&lt;/p&gt;

&lt;p&gt;There have already been a series of attempts to identify specific circumstances where T&amp;amp;A projects have an impact (see&lt;a href=&quot;https://www.ids.ac.uk/files/dmfile/Wp383.pdf&quot;&gt; here&lt;/a&gt;,&lt;a href=&quot;http://www.transparency-initiative.org/wp-content/uploads/2011/05/impact_case_studies_final1.pdf&quot;&gt; here&lt;/a&gt;,&lt;a href=&quot;http://www.transparency-initiative.org/news/civil-society-for-development-2&quot;&gt; here&lt;/a&gt; and &lt;a href=&quot;http://www.transparency-initiative.org/wp-content/uploads/2013/10/Think-Piece-Guerzovich-Shaw.pdf&quot;&gt;here&lt;/a&gt; for some longer reads). All of them call for a bigger, more solid evidence base, which could allow organisations to keep building the long-term pressure for transparency and accountability that’s essential to many of these projects. Now, the main push is to design projects specifically so that they produce useful evidence about the project itself and T&amp;amp;A projects more generally. A whole range of people are testing out how to improve the way we do things, but it can be tricky to work out who’s doing what. This post rounds up the main initiatives and identifies a few trends.&lt;/p&gt;

&lt;p&gt;Measuring things better also involves thinking carefully about what ‘impact’ means in reality. For example, projects might not record developments in a way that&lt;a href=&quot;http://r4d.dfid.gov.uk/PDF/Outputs/Mis_SPC/60827_DPRGaventaMcGee_Preprint.pdf&quot;&gt; reflects smaller, intermediate changes&lt;/a&gt; that could add up to a deeper shift over time. Programme staff don’t always&lt;a href=&quot;https://www.ids.ac.uk/idspublication/so-what-difference-does-it-make-mapping-the-outcomes-of-citizen-engagement1&quot;&gt; think through or articulate their assumptions&lt;/a&gt; about outcomes when drawing up a theory of change – if, indeed, they write one at all. There’s also debate about &lt;a href=&quot;http://www.transparency-initiative.org/wp-content/uploads/2014/01/TP.International-TA-interventions.Fox_.pdf&quot;&gt;how much impact we can reasonably expect&lt;/a&gt; – especially from projects in complex political environments – as well as calls to&lt;a href=&quot;http://www.transparency-initiative.org/wp-content/uploads/2014/05/Thinking-and-Working-Politically.May-2014.pdf&quot;&gt; better understand a particular political context&lt;/a&gt; before deciding what ‘success’ there might look like.&lt;/p&gt;

&lt;p&gt;We’ve noticed a few more trends in the projects detailed below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Academics are working closely with the people who implement projects, designing interventions so that the evidence they create contributes to our general understanding of T&amp;amp;A initiatives.&lt;/li&gt;
  &lt;li&gt;Projects are increasingly trying to use methods that take account of the complex environment in which a project operates, with some mixing quantitative and qualitative methods and others using real-time assessments and other methods that allow them to factor in changing political dynamics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall, we’re excited to see this issue being addressed from so many perspectives. Here’s our round-up:&lt;/p&gt;

&lt;h4 id=&quot;making-all-voices-countmavc&quot;&gt;&lt;a href=&quot;http://www.makingallvoicescount.org/&quot;&gt;Making All Voices Count&lt;/a&gt; (MAVC)&lt;/h4&gt;

&lt;p&gt;MAVC’s &lt;a href=&quot;http://www.makingallvoicescount.org/learning/&quot;&gt;Learning and Evidence component&lt;/a&gt;, an applied and practical fund for research and learning on tech-for-T&amp;amp;A, focuses on answering the question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‘Is closing the feedback loop between citizens and their government a catalytic force that enables better governance, enhances service delivery and strengthens democracy?’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.ids.ac.uk/&quot;&gt;Institute for Development Studies&lt;/a&gt; (IDS), which is managing the component, is building an evidence base on technology and open government and has recommended a ‘solid meta-level review…as soon as sufficient evidence is available’. &lt;a href=&quot;http://www.makingallvoicescount.org/publications/&quot;&gt;Four briefing papers&lt;/a&gt; were published in July, including &lt;a href=&quot;http://www.makingallvoicescount.org/publications/count/&quot;&gt;a ‘selective, purposive’ review&lt;/a&gt; of recent work in this area.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What methods is it using?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Learning and Evidence component will distribute research grants on specific themes (ranging from small, bounded case studies to broader multi-level approaches), and integrate them with the projects supported by MAVC. In this way, projects will be specifically designed so that they create new, useful evidence – while being informed by new ideas generated by the Learning and Evidence component.&lt;/p&gt;

&lt;p&gt;The component is reviewing existing evidence with input from a wide range of actors, holding a series of meetings, &lt;a href=&quot;http://www.makingallvoicescount.org/blog/feedback-loops/&quot;&gt;e-dialogues&lt;/a&gt; and discussions with groups involved in transparency and accountability while surveying academic publications and ‘grey’ organisational literature. The information will be accessible through an online &lt;a href=&quot;http://www.makingallvoicescount.org/knowledge-repository/&quot;&gt;Knowledge Repository &lt;/a&gt;(which is also inviting contributions from individuals and organisations outside the initiative).&lt;/p&gt;

&lt;h4 id=&quot;transparency-and-accountability-initiativetai&quot;&gt;&lt;a href=&quot;http://www.transparency-initiative.org/&quot;&gt;Transparency and Accountability Initiative&lt;/a&gt; (T/AI)&lt;/h4&gt;

&lt;p&gt;T/AI is focused on four questions in relation to tech-for-T&amp;amp;A:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How and when technological innovations can be transferred to other contexts?&lt;/li&gt;
  &lt;li&gt;How is information used and consumed; how and why do citizens engage; and how can new technologies for T&amp;amp;A lead to empowerment, accountability and development outcomes?&lt;/li&gt;
  &lt;li&gt;How and when new technologies can perpetuate power inequalities?&lt;/li&gt;
  &lt;li&gt;How can we improve the diagnosis of context and theories of action?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;What methods is it using?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Members of &lt;a href=&quot;http://www.transparency-initiative.org/news/talearners-safe-space&quot;&gt;TALEARN&lt;/a&gt;, a community of practice working on governance facilitated by T/AI, have long been discussing impact and evaluation. Rather than focusing on specific methods itself, TALEARN is collecting and systematising existing evidence about impact. It’s doing this in three main ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Building an online knowledge repository to consolidate and link evidence on impact: TALEARN’s Accountability and Participation (TAP) Nexus Practice Group, together with the &lt;a href=&quot;http://www.u4.no/&quot;&gt;U4 Anti-Corruption Resource Centre&lt;/a&gt;, is currently building an online &lt;a href=&quot;http://www.transparency-initiative.org/wp-content/uploads/2014/05/TAP-Knowledge-Repository-Concept-Note.pdf&quot;&gt;Knowledge Repository&lt;/a&gt; that will hold practical information from projects as well as more formally structured information. It also aims to identify evidence gaps and methods for filling them.&lt;/li&gt;
  &lt;li&gt;Holding meetings for a wide range of people interested in measuring impact: TALEARN regularly holds meetings in which practitioners, donors and researchers can exchange ideas. The most recent of its annual meetings, in &lt;a href=&quot;http://www.transparency-initiative.org/wp-content/uploads/2014/03/TALEARN-Jakarta-workshop-report.pdf&quot;&gt;Jakarta in March 2014&lt;/a&gt;, looked at impact in some depth.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Publishing think pieces on new developments in the field: for example, one recent piece has assessed recent calls for transparency projects to &lt;a href=&quot;http://www.transparency-initiative.org/wp-content/uploads/2014/05/Thinking-and-Working-Politically.May-2014.pdf&quot;&gt;‘work politically&lt;/a&gt;‘.&lt;/p&gt;

&lt;h4 id=&quot;transparency-for-development-t4d-project&quot;&gt;&lt;a href=&quot;http://www.transparency-initiative.org/news/harvard-kennedy-school-awarded-8-1-million&quot;&gt;Transparency for Development (T4D) Project&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This five-year, $8.1 million multi-country &lt;a href=&quot;http://www.transparency-initiative.org/news/harvard-kennedy-school-awarded-8-1-million&quot;&gt;research project&lt;/a&gt; is being jointly undertaken by &lt;a href=&quot;http://r4d.org/&quot;&gt;Results for Development Institute&lt;/a&gt; (R4D) and the &lt;a href=&quot;http://www.ash.harvard.edu/&quot;&gt;Harvard Kennedy School (Ash Center for Democratic Governance and Innovation)&lt;/a&gt;. It’s examining community transparency and accountability initiatives and their effect on the health and social sector. But it’s even more ambitious than that: it’s trying to produce evidence to generate a broader understanding of the impact of T&amp;amp;A initiatives at large. It has specifically selected countries and types of interventions to help provide evidence that can be used to make generalised statements about where T&amp;amp;A projects tend to work best.&lt;/p&gt;

&lt;p&gt;Researchers from &lt;a href=&quot;http://r4d.org/&quot;&gt;R4D&lt;/a&gt; and the &lt;a href=&quot;http://www.ash.harvard.edu/&quot;&gt;Ash Center&lt;/a&gt; will work with local civil society groups simultaneously in several countries to co-design a transparency and accountability intervention for health care, and then assess the results. The study is currently evaluating interventions in 200 communities in Indonesia and Tanzania as part of a pilot phase, and plans to expand to other countries in the coming years.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What methods is it using?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The project will use mixed methods, combining qualitative field research with a randomised control trial study – the &lt;a href=&quot;http://r4d.org/focus-areas/transparency-development&quot;&gt;first time&lt;/a&gt; this has been attempted simultaneously in more than one country. The main goal is to provide evidence that both people working on the ground and academics can use to improve health, accountability and citizen participation. The project has two phases:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase I:&lt;/strong&gt; a mixed methods approach in Tanzania and Indonesia using randomised control trials (RCTs) to evaluate effects on health care quality/outcomes and community power relations, combined with qualitative studies – direct observation, focus groups, informant interviews and ethnographic methods – to assess the role of local context and produce more data on what mechanisms lead to impact.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase II:&lt;/strong&gt; researchers will assess whether the intervention can be scaled and generalised beyond Indonesia and Tanzania. They’ll also look at what T/A interventions are appropriate for different sets of social, political, and economic circumstances. If results show that there is potential to apply the findings more broadly, a similar intervention will be tested in other countries. If Phase I interventions had little impact, Phase II will redesign new interventions that take this into account.&lt;/p&gt;

&lt;p&gt;Here’s a &lt;a href=&quot;http://www.ash.harvard.edu/extension/ash/docs/T4D_Project_Summary.pdf&quot;&gt;detailed project outline&lt;/a&gt; with more detail on all of this. (The project is jointly funded by the &lt;a href=&quot;http://www.hewlett.org/&quot;&gt;William and Flora Hewlett Foundation&lt;/a&gt;, &lt;a href=&quot;http://www.dfid.gov.uk/&quot;&gt;DFID&lt;/a&gt; and the &lt;a href=&quot;http://www.gatesfoundation.org/&quot;&gt;Bill and Melinda Gates Foundation&lt;/a&gt; – and is brokered under T/AI).&lt;/p&gt;

&lt;h4 id=&quot;open-government-partnershipogp&quot;&gt;&lt;a href=&quot;http://www.opengovpartnership.org/&quot;&gt;Open Government Partnership&lt;/a&gt; (OGP)&lt;/h4&gt;

&lt;p&gt;Technology plays a key role in the &lt;a href=&quot;http://www.opengovpartnership.org/&quot;&gt;OGP&lt;/a&gt;, a multilateral initiative in which governments commit to promoting transparency, combating corruption and promoting new technologies through a declaration, followed by an action plan developed with public consultation.&lt;/p&gt;

&lt;p&gt;It’s often difficult to use the OGP data to gauge the impact of tech for T&amp;amp;A because each country creates its own action plans, meaning that commitments aren’t always directly comparable. Commitments about Open Data and e-government make up almost one-third of all those included in the action plans, although there that these are not always explicitly linked to the broader reform efforts needed.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What methods is it using?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Impact in the OGP is determined by an independent body called the &lt;a href=&quot;http://www.opengovpartnership.org/about/about-irm&quot;&gt;Independent Review Mechanism&lt;/a&gt; (IRM), which monitors countries’ progress against their action plans and according to the OGP’s stated values. IRM reviewers also assess progress by giving a qualitative judgement on whether progress on each element of a country’s action plan has social impact, as well as the extent to which it’s been completed. They can also use a ‘star’ rating that singles out a government commitment that they consider has significant social impact, is complete (or substantially complete), and is relevant to OGP values (of the 35 most recent countries to have completed OGP action plans, &lt;a href=&quot;http://tisne.org/2014/05/27/the-ambition-of-open-government-partnership/&quot;&gt;24.7% were starred&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The OGP has created a comparative qualitative and quantitative &lt;a href=&quot;http://www.opengovpartnership.org/about/about-irm&quot;&gt;database&lt;/a&gt; of the data collected since it started in 2011. The information gathered has yet to be used in contexts beyond the OGP, though &lt;a href=&quot;http://www.makingallvoicescount.org/assets/MAVC_GOVERNMENT_RESPONSIVENESS.pdf&quot;&gt;some researchers&lt;/a&gt; have seen potential to do so.&lt;/p&gt;

&lt;h4 id=&quot;standardsindicators-on-open-government-and-transparency&quot;&gt;Standards/indicators on open government and transparency&lt;/h4&gt;

&lt;p&gt;There’s a whole range of efforts to assess and rank government openness by measuring it according to sets of indicators – in fact, far too many to list here. These indicators typically include a mixture of objective measures (laws, signatures of treaties and so on) and subjective measures like responses to questionnaires or expert assessments. UNDP have a &lt;a href=&quot;http://www.undg.org/docs/11652/UNDP-Governance-Indicators-Guide-(2007).pdf&quot;&gt;useful guide&lt;/a&gt; that explains the main uses and limitations of different indicators.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.freedominfo.org/2012/10/measuring-openness-a-survey-of-transparency-ratings-and-the-prospects-for-a-global-index&quot;&gt;This report&lt;/a&gt; by Sheila Coronel provides an excellent sample of the key indices focusing on right-to-information. As she explains, lack of evidence isn’t the problem this time round:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‘The problem is not that transparency has not been measured enough: It has. But what we have today is a patchwork of ratings and indices evaluating various aspects of government openness. These measures cover different sets of countries, examine different spheres of government transparency, and use a variety of criteria and methodologies.’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are good practical reasons for this: for a start, it would be prohibitively expensive to cover over 100 countries, and not all countries have enough local researchers to provide information for the range of indicators out there. Still, it makes it hard to assess the field as a whole, and still harder to make general statements about what it’s contributing to our understanding of impact on T&amp;amp;A. Interestingly, there’s debate about whether and to what extent transparency indices and ratings actually lead to reforms themselves – another area where there’s a need for further research.&lt;/p&gt;

&lt;p&gt;One initiative particularly worth mentioning here is &lt;a href=&quot;http://www.opengovstandards.org/&quot;&gt;Open Government Standards&lt;/a&gt;, which attempts to define what ‘open government’ means from civil society’s point of view. The initiative, coordinated by &lt;a href=&quot;http://www.access-info.org/&quot;&gt;Access Info Europe&lt;/a&gt;, identifies specific measures that an open government policy should contain – distinguishing them from concepts such as good government or e-government. Open Government Standards began in 2012, following criticism that the first OGP Action Plans of 2012 did not all relate to each other. For more detailed explanations of the standards, see &lt;a href=&quot;http://www.access-info.org/documents/Access_Docs/Advancing/OGD/Transparency_Standards12072013.pdf&quot;&gt;Transparency&lt;/a&gt;, &lt;a href=&quot;http://www.access-info.org/documents/Access_Docs/Advancing/OGD/Participation_Standards11072013.pdf&quot;&gt;Participation&lt;/a&gt; and &lt;a href=&quot;http://www.access-info.org/documents/Access_Docs/Advancing/OGD/Accountability_Standards11072013.pdf&quot;&gt;Accountability&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;international-budget-partnership&quot;&gt;International Budget Partnership&lt;/h4&gt;

&lt;p&gt;In the area of budget advocacy, the &lt;a href=&quot;http://internationalbudget.org/&quot;&gt;International Budget Partnership&lt;/a&gt; (IBP) has done a lot of work on understanding and assessing impact, as part of its ‘long-term commitment to learning about how, when and under what circumstances CSOs’ budget advocacy has impact’. As well as producing a &lt;a href=&quot;http://internationalbudget.org/publications/the-super-duper-impact-planning-guide/&quot;&gt;Super-Duper Impact Planning Guide&lt;/a&gt; to help organisations design full, considered impact plans, it’s undertaken a series of four-year &lt;a href=&quot;http://blogs.worldbank.org/publicsphere/ups-and-downs-struggle-accountability-four-new-real-time-studies&quot;&gt;real-time evaluations&lt;/a&gt; of its projects from 2009 to 2013. The resulting case studies (see here for full-length versions for &lt;a href=&quot;http://internationalbudget.org/publications/health-citizenship-and-human-rights-advocacy-initiative-improving-access-to-health-services-in-mexico/&quot;&gt;Mexico&lt;/a&gt;, &lt;a href=&quot;http://internationalbudget.org/publications/the-art-of-getting-in-the-way-five-years-of-the-bndes-platform-2/&quot;&gt;Brazil&lt;/a&gt;, &lt;a href=&quot;http://internationalbudget.org/publications/when-opportunity-beckons-the-impact-of-the-public-service-accountability-monitors-work-on-improving-health-budgets-in-south-africa/&quot;&gt;South Africa&lt;/a&gt; and &lt;a href=&quot;http://internationalbudget.org/publications/raising-the-stakes-the-impact-of-hakielimus-advocacy-work-on-education-policy-and-budget-in-tanzania/&quot;&gt;Tanzania&lt;/a&gt;) provide a different perspective on the fluid, often unpredictable nature of T&amp;amp;A projects and a more nuanced way of viewing impact.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What methods does it use?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The IBP appointed a team of external researchers to monitor advocacy campaigns in each of the four countries throughout their life-span, for a three-year period.  The studies typically used primary research, such as interviews and government documents, alongside secondary sources, like published reports and articles.&lt;/p&gt;

&lt;p&gt;For example, the team writing the case study for the Tanzania project (assessing the impact of advocacy on to the education sector by the NGO HakiElimu) used a longitudinal method that attempted to assess not only what had happened during the three-year period but why and how it had happened. They also used contribution analysis, a method of determining to what extent observed results are due to program activities rather than other factors.&lt;/p&gt;

&lt;p&gt;To do so, the case study team interviewed and held focus groups with HakiElimu staff members and others linked to the education sector in Tanzania to get their take on what changes had occurred during the whole project period. The team then asked interviewees for their explanation for the changes, considering both the impact of civil society and HakiElimu, as well as other possible explanations. Drawing on a review of secondary evidence, they attempted to test or substantiate both the observations of change, as well as the explanations for why they occurred.&lt;/p&gt;

&lt;p&gt;It’s going to be a long haul before we have a reliable evidence base for what works and what doesn’t, and we’re looking forward to seeing how the projects and activists doing front-line advocacy work can contribute to and benefit from that work. This post is an effort to get an overview of the field, and is therefore unlikely to be completely comprehensive. &lt;/p&gt;</content><author><name>Tom Walker</name></author><category term="technology, research" /><summary type="html">For the last couple of years, the engine room has been exploring how to measure whether technology for transparency and accountability (tech-for-T&amp;amp;A) projects actually have an impact. It’s an area filled with lengthy evaluation reports and endless discussions on methodologies, but few solid conclusions. Dip even a cautious toe into these murky waters and you’ll find a researcher complaining that the evidence available is ‘weak’ or ‘uneven, piecemeal and scattered‘.</summary></entry></feed>